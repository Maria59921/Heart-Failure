{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdJYvG107ISN",
        "outputId": "553717c7-b11f-45c6-dde7-137b4708687b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting seclea-ai==1.0.2\n",
            "  Downloading seclea_ai-1.0.2-py3-none-any.whl (281 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m281.4/281.4 KB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from seclea-ai==1.0.2) (4.4.2)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.8/dist-packages (from seclea-ai==1.0.2) (1.3.5)\n",
            "Requirement already satisfied: dill>=0.3.4 in /usr/local/lib/python3.8/dist-packages (from seclea-ai==1.0.2) (0.3.6)\n",
            "Collecting pickleDB>=0.9.2\n",
            "  Downloading pickleDB-0.9.2.tar.gz (3.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.8/dist-packages (from seclea-ai==1.0.2) (1.14.1)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from seclea-ai==1.0.2) (2.25.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.3.0->seclea-ai==1.0.2) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.3.0->seclea-ai==1.0.2) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.3.0->seclea-ai==1.0.2) (2022.7.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.0.0->seclea-ai==1.0.2) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.0.0->seclea-ai==1.0.2) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.0.0->seclea-ai==1.0.2) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.0.0->seclea-ai==1.0.2) (1.24.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas>=1.3.0->seclea-ai==1.0.2) (1.15.0)\n",
            "Building wheels for collected packages: pickleDB\n",
            "  Building wheel for pickleDB (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pickleDB: filename=pickleDB-0.9.2-py3-none-any.whl size=4269 sha256=0d9bef5586e4b83073d5f0893c57ad73d6df461c66afbf2982c4045b98cf90a6\n",
            "  Stored in directory: /root/.cache/pip/wheels/88/91/d4/ef2e6a46ad2bc41f9cfad35fa2db5b34357a5e4da67c385ffa\n",
            "Successfully built pickleDB\n",
            "Installing collected packages: pickleDB, seclea-ai\n",
            "Successfully installed pickleDB-0.9.2 seclea-ai-1.0.2\n",
            "/bin/bash: conda: command not found\n"
          ]
        }
      ],
      "source": [
        "!pip install seclea-ai==1.0.2\n",
        "!conda install seclea-ai==1.0.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from seclea_ai import SecleaAI\n",
        "\n",
        "# NOTE - use the organization name provided to you when issued credentials.\n",
        "seclea = SecleaAI(project_name=\"HeartFailure\", organization='Seclea')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfQ8JQT67OBo",
        "outputId": "a03b022f-21ef-47da-d8fc-132cc632dd8e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Username: mariaantony\n",
            "Password: 路路路路路路路路路路\n",
            "success\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# load the data\n",
        "data = pd.read_csv('/content/heart_failure_clinical_records_dataset.csv')\n",
        "\n",
        "# define the metadata for the dataset.\n",
        "dataset_metadata = {\"outcome_name\": \"DEATH_EVENT\",\n",
        "                    \"favourable_outcome\": \"1\",\n",
        "                    \"unfavourable_outcome\": \"0\",\n",
        "                    \"continuous_features\": [\n",
        "                                            \"age\",\n",
        "                                            'anaemia',\n",
        "                                            'creatinine_phosphokinase',\n",
        "                                            'diabetes',\n",
        "                                            'ejection_fraction',\n",
        "                                            'high_blood_pressure',\n",
        "                                            'platelets',\n",
        "                                            'serum_creatinine',\n",
        "                                            'serum_sodium',\n",
        "                                            'sex',\n",
        "                                            'smoking',\n",
        "                                            'time',\n",
        "                                            'DEATH_EVENT',\n",
        "                                            ]}\n",
        "\n",
        "\n",
        "# 猬锔 upload the dataset - pick a meaningful name here, you'll be seeing it a lot on the platform!\n",
        "seclea.upload_dataset(dataset=data, dataset_name=\"Heart Failure\", metadata=dataset_metadata)"
      ],
      "metadata": {
        "id": "9DTracGs75og"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a copy to isolate the original dataset\n",
        "df1 = data.copy(deep=True)\n",
        "\n",
        "def encode_nans(df):\n",
        "    # convert the special characters to nans\n",
        "    return df.replace('?', np.NaN)\n",
        "\n",
        "df2 = encode_nans(df1)"
      ],
      "metadata": {
        "id": "RPSPkec0ASFW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Drop the the column which are more than some proportion NaN values\n",
        "def drop_nulls(df, threshold):\n",
        "    cols = [x for x in df.columns if df[x].isnull().sum() / df.shape[0] > threshold]\n",
        "    return df.drop(columns=cols)\n",
        "\n",
        "# We choose 95% as our threshold\n",
        "null_thresh = 0.95\n",
        "df3 = drop_nulls(df2, threshold=null_thresh)\n",
        "\n",
        "def drop_correlated(data, thresh):\n",
        "    import numpy as np\n",
        "\n",
        "    # calculate correlations\n",
        "    corr_matrix = data.corr().abs()\n",
        "    # get the upper part of correlation matrix\n",
        "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
        "\n",
        "    # columns with correlation above threshold\n",
        "    redundant = [column for column in upper.columns if any(upper[column] >= thresh)]\n",
        "    print(f\"Columns to drop with correlation > {thresh}: {redundant}\")\n",
        "    new_data = data.drop(columns=redundant)\n",
        "    return new_data\n",
        "\n",
        "# drop columns that are too closely correlated\n",
        "correlation_threshold = 0.95\n",
        "df4 = drop_correlated(df3, correlation_threshold)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PH93n9fhAZRN",
        "outputId": "de12a781-ac39-4fe1-bbc5-6a9e99002053"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns to drop with correlation > 0.95: []\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-55dc1374a23b>:16: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from seclea_ai.transformations import DatasetTransformation\n",
        "\n",
        "\n",
        "# define the updates to the metadata - only changes are updated - here a continuous feature has been dropped so now\n",
        "# we remove it from the list of continuous features.\n",
        "processed_metadata = {\"continuous_features\": [\n",
        "                                            \"age\",\n",
        "                                            'anaemia',\n",
        "                                            'creatinine_phosphokinase',\n",
        "                                            'diabetes',\n",
        "                                            'ejection_fraction',\n",
        "                                            'high_blood_pressure',\n",
        "                                            'platelets',\n",
        "                                            'serum_creatinine',\n",
        "                                            'serum_sodium',\n",
        "                                            'sex',\n",
        "                                            'smoking',\n",
        "                                            'time',\n",
        "                                            'DEATH_EVENT',\n",
        "                                            ]}\n",
        "\n",
        "#  define the transformations - note the arguments\n",
        "cleaning_transformations = [\n",
        "            DatasetTransformation(encode_nans, data_kwargs={\"df\": df1}, kwargs={}, outputs=[\"df\"]),\n",
        "            DatasetTransformation(\n",
        "                drop_nulls, data_kwargs={\"df\": \"inherit\"}, kwargs={\"threshold\": null_thresh}, outputs=[\"data\"]\n",
        "            ),\n",
        "            DatasetTransformation(\n",
        "                drop_correlated, data_kwargs={\"data\": \"inherit\"}, kwargs={\"thresh\": correlation_threshold}, outputs=[\"df\"]\n",
        "            ),\n",
        "        ]\n",
        "\n",
        "\n",
        "\n",
        "def fill_nan_const(df, val):\n",
        "    \"\"\"Fill NaN values in the dataframe with a constant value\"\"\"\n",
        "    return df.replace(['None', np.nan], val)\n",
        "\n",
        "\n",
        "# Fill nans in 1st dataset with -1\n",
        "const_val = -1\n",
        "df_const = fill_nan_const(df4, const_val)\n",
        "\n",
        "def fill_nan_mode(df, columns):\n",
        "    \"\"\"\n",
        "    Fills nans in specified columns with the mode of that column\n",
        "    Note that we want to make sure to not modify the dataset we passed in but to\n",
        "    return a new copy.\n",
        "    We do that by making a copy and specifying deep=True.\n",
        "    \"\"\"\n",
        "    new_df = df.copy(deep=True)\n",
        "    for col in df.columns:\n",
        "        if col in columns:\n",
        "            new_df[col] = df[col].fillna(df[col].mode()[0])\n",
        "    return new_df\n",
        "\n",
        "nan_cols = ['high_blood_pressure','platelets', 'serum_creatinine']\n",
        "df_mode = fill_nan_mode(df4, nan_cols)\n",
        "\n",
        "\n",
        "\n",
        "# find columns with categorical data for both dataset\n",
        "cat_cols = df_const.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "def encode_categorical(df, cat_cols):\n",
        "  from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "  new_df = df.copy(deep=True)\n",
        "  for col in cat_cols:\n",
        "    if col in df.columns:\n",
        "        le = LabelEncoder()\n",
        "        le.fit(list(df[col].astype(str).values))\n",
        "        new_df[col] = le.transform(list(df[col].astype(str).values))\n",
        "  return new_df\n",
        "\n",
        "df_const = encode_categorical(df_const, cat_cols)\n",
        "df_mode = encode_categorical(df_mode, cat_cols)\n",
        "\n",
        "# Update metadata with new encoded values for the outcome column.\n",
        "encoded_metadata = {\"favourable_outcome\": 0,\n",
        "                    \"unfavourable_outcome\": 1,}\n",
        "\n",
        "\n",
        "#  define the transformations - for the constant fill dataset\n",
        "const_processed_transformations = [\n",
        "    DatasetTransformation(fill_nan_const, data_kwargs={\"df\": df4}, kwargs={\"val\": const_val}, outputs=[\"df\"]),\n",
        "    DatasetTransformation(encode_categorical, data_kwargs={\"df\": \"inherit\"}, kwargs={\"cat_cols\":cat_cols}, outputs=[\"df\"]),\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "#  define the transformations - for the mode fill dataset\n",
        "mode_processed_transformations = [\n",
        "    DatasetTransformation(fill_nan_mode, data_kwargs={\"df\": df4}, kwargs={\"columns\": nan_cols}, outputs=[\"df\"]),\n",
        "    DatasetTransformation(encode_categorical, data_kwargs={\"df\": \"inherit\"}, kwargs={\"cat_cols\": cat_cols}, outputs=[\"df\"]),\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "def get_samples_labels(df, output_col):\n",
        "    X = df.drop(output_col, axis=1)\n",
        "    y = df[output_col]\n",
        "\n",
        "    return X, y\n",
        "\n",
        "# split the datasets into samples and labels ready for modelling.\n",
        "X_const, y_const = get_samples_labels(df_const, \"DEATH_EVENT\")\n",
        "X_mode, y_mode = get_samples_labels(df_mode, \"DEATH_EVENT\")\n",
        "\n",
        "def get_test_train_splits(X, y, test_size, random_state):\n",
        "    from sklearn.model_selection import train_test_split\n",
        "\n",
        "    return train_test_split(\n",
        "        X, y, test_size=test_size, stratify=y, random_state=random_state\n",
        "    )\n",
        "    # returns X_train, X_test, y_train, y_test\n",
        "\n",
        "# split into test and train sets\n",
        "X_train_const, X_test_const, y_train_const, y_test_const = get_test_train_splits(X_const, y_const, test_size=0.2, random_state=42)\n",
        "X_train_mode, X_test_mode, y_train_mode, y_test_mode = get_test_train_splits(X_mode, y_mode, test_size=0.2, random_state=42)\n",
        "\n",
        "#  define the transformations - for the constant fill training set\n",
        "const_train_transformations = [\n",
        "    DatasetTransformation(\n",
        "            get_test_train_splits,\n",
        "            data_kwargs={\"X\": X_const, \"y\": y_const},\n",
        "            kwargs={\"test_size\": 0.2, \"random_state\": 42},\n",
        "            outputs=[\"X_train_const\", None, \"y_train_const\", None],\n",
        "            split=\"train\",\n",
        "            ),\n",
        "]\n",
        "\n",
        "# 猬锔 upload the const fill training set\n",
        "#seclea.upload_dataset_split(\n",
        " #                       X=X_train_const,\n",
        "  #                      y=y_train_const,\n",
        "   #                     dataset_name=\"Heart failure - Const Fill - Train\",\n",
        "    #                    metadata={},\n",
        "     #                   transformations=const_train_transformations\n",
        "#)\n",
        "\n",
        "#  define the transformations - for the constant fill test set\n",
        "const_test_transformations = [\n",
        "    DatasetTransformation(\n",
        "            get_test_train_splits,\n",
        "            data_kwargs={\"X\": X_const, \"y\": y_const},\n",
        "            kwargs={\"test_size\": 0.2, \"random_state\": 42},\n",
        "            outputs=[None, \"X_test_const\", None, \"y_test_const\"],\n",
        "            split=\"test\"\n",
        "            ),\n",
        "]\n",
        "\n",
        "# 猬锔 upload the const fill test set\n",
        "#seclea.upload_dataset_split(X=X_test_const,\n",
        "#                     y=y_test_const,\n",
        " #                     dataset_name=\"Heart failure - Const Fill - Test\",\n",
        "  #                    metadata={},\n",
        "   #                   transformations=const_test_transformations)\n",
        "\n",
        "#  define the transformations - for the mode fill training set\n",
        "mode_train_transformations = [\n",
        "    DatasetTransformation(\n",
        "            get_test_train_splits,\n",
        "            data_kwargs={\"X\": X_mode, \"y\": y_mode},\n",
        "            kwargs={\"test_size\": 0.2, \"random_state\": 42},\n",
        "            outputs=[\"X_train_mode\", None, \"y_train_mode\", None],\n",
        "            split=\"train\",\n",
        "            ),\n",
        "]\n",
        "\n",
        "# 猬锔 upload the mode fill train set\n",
        "#seclea.upload_dataset_split(X=X_train_mode,\n",
        " #                     y=y_train_mode,\n",
        "  #                    dataset_name=\"Heart failure - Mode Fill - Train\",\n",
        "   #                   metadata=processed_metadata,\n",
        "    #                  transformations=mode_train_transformations)\n",
        "\n",
        "#  define the transformations - for the mode fill test set\n",
        "mode_test_transformations = [\n",
        "    DatasetTransformation(\n",
        "            get_test_train_splits,\n",
        "            data_kwargs={\"X\": X_mode, \"y\": y_mode},\n",
        "            kwargs={\"test_size\": 0.2, \"random_state\": 42},\n",
        "            outputs=[None, \"X_test_mode\", None, \"y_test_mode\"],\n",
        "            split=\"test\",\n",
        "            ),\n",
        "]\n",
        "\n",
        "# 猬锔 upload the mode fill test set\n",
        "#seclea.upload_dataset_split(X=X_test_mode,\n",
        " #                     y=y_test_mode,\n",
        "  #                    dataset_name=\"Heart failure - Mode Fill - Test\",\n",
        "   #                   metadata={},\n",
        "    #                  transformations=mode_test_transformations)\n",
        "\n",
        "\n",
        "\n",
        "def smote_balance(X, y, random_state):\n",
        "    from imblearn.over_sampling import SMOTE\n",
        "\n",
        "    sm = SMOTE(random_state=random_state)\n",
        "\n",
        "    X_sm, y_sm = sm.fit_resample(X, y)\n",
        "\n",
        "    print(\n",
        "        f\"\"\"Shape of X before SMOTE: {X.shape}\n",
        "    Shape of X after SMOTE: {X_sm.shape}\"\"\"\n",
        "    )\n",
        "    print(\n",
        "        f\"\"\"Shape of y before SMOTE: {y.shape}\n",
        "    Shape of y after SMOTE: {y_sm.shape}\"\"\"\n",
        "    )\n",
        "    return X_sm, y_sm\n",
        "    # returns X, y\n",
        "\n",
        "# balance the training sets - creating new training sets for comparison\n",
        "X_train_const_smote, y_train_const_smote = smote_balance(X_train_const, y_train_const, random_state=42)\n",
        "X_train_mode_smote, y_train_mode_smote = smote_balance(X_train_mode, y_train_mode, random_state=42)\n",
        "\n",
        "#  define the transformations - for the constant fill balanced train set\n",
        "const_smote_transformations = [\n",
        "    DatasetTransformation(\n",
        "            smote_balance,\n",
        "            data_kwargs={\"X\": X_train_const, \"y\": y_train_const},\n",
        "            kwargs={\"random_state\": 42},\n",
        "            outputs=[\"X\", \"y\"]\n",
        "            ),\n",
        "]\n",
        "\n",
        "# 猬锔 upload the constant fill balanced train set\n",
        "#seclea.upload_dataset_split(X=X_train_const_smote,\n",
        " #                     y=y_train_const_smote,\n",
        "  #                    dataset_name=\"Heart failure - Const Fill - Smote Train\",\n",
        "   #                   metadata={},\n",
        "    #                  transformations=const_smote_transformations)\n",
        "\n",
        "#  define the transformations - for the mode fill balanced train set\n",
        "mode_smote_transformations = [\n",
        "    DatasetTransformation(\n",
        "            smote_balance,\n",
        "            data_kwargs={\"X\": X_train_mode, \"y\": y_train_mode},\n",
        "            kwargs={\"random_state\": 42},\n",
        "            outputs=[\"X\", \"y\"]\n",
        "            ),\n",
        "]\n",
        "\n",
        "# 猬锔 upload the mode fill balanced train set\n",
        "#seclea.upload_dataset_split(X=X_train_mode_smote,\n",
        " #                     y=y_train_mode_smote,\n",
        "  #                    dataset_name=\"Heart failure - Mode Fill - Smote Train\",\n",
        "   #                   metadata={},\n",
        "    #                  transformations=mode_smote_transformations)"
      ],
      "metadata": {
        "id": "TR-sbHDuBYwb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c8565ee-da42-41c9-f8bc-f7b5c6cb11f1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X before SMOTE: (239, 12)\n",
            "    Shape of X after SMOTE: (324, 12)\n",
            "Shape of y before SMOTE: (239,)\n",
            "    Shape of y after SMOTE: (324,)\n",
            "Shape of X before SMOTE: (239, 12)\n",
            "    Shape of X after SMOTE: (324, 12)\n",
            "Shape of y before SMOTE: (239,)\n",
            "    Shape of y after SMOTE: (324,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "classifiers = {\n",
        "    \"RandomForestClassifier\": RandomForestClassifier(),\n",
        "    \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n",
        "    \"GradientBoostingClassifier\": GradientBoostingClassifier()\n",
        "}\n",
        "\n",
        "datasets = [\n",
        "    (\"Const Fill\", (X_train_const, X_test_const, y_train_const, y_test_const)),\n",
        "    (\"Mode Fill\", (X_train_mode, X_test_mode, y_train_mode, y_test_mode)),\n",
        "    (\"Const Fill Smote\", (X_train_const_smote, X_test_const, y_train_const_smote, y_test_const)),\n",
        "    (\"Mode Fill Smote\", (X_train_mode_smote, X_test_mode, y_train_mode_smote, y_test_mode))\n",
        "    ]\n",
        "\n",
        "for name, (X_train, X_test, y_train, y_test) in datasets:\n",
        "\n",
        "    for key, classifier in classifiers.items():\n",
        "        # cross validate to get an idea of generalisation.\n",
        "        training_score = cross_val_score(classifier, X_train, y_train, cv=5)\n",
        "\n",
        "        # train on the full training set\n",
        "        classifier.fit(X_train, y_train)\n",
        "\n",
        "        # 猬锔 upload the fully trained model\n",
        "       # seclea.upload_training_run_split(model=classifier, X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test)\n",
        "\n",
        "        # test accuracy\n",
        "        y_preds = classifier.predict(X_test)\n",
        "        test_score = accuracy_score(y_test, y_preds)\n",
        "        print(f\"Classifier: {classifier.__class__.__name__} has a training score of {round(training_score.mean(), 3) * 100}% accuracy score on {name}\")\n",
        "        print(f\"Classifier: {classifier.__class__.__name__} has a test score of {round(test_score, 3) * 100}% accuracy score on {name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ud7QjztlLjYH",
        "outputId": "4884f009-c2df-44a7-892f-e5ad850c5a87"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classifier: RandomForestClassifier has a training score of 85.39999999999999% accuracy score on Const Fill\n",
            "Classifier: RandomForestClassifier has a test score of 83.3% accuracy score on Const Fill\n",
            "Classifier: DecisionTreeClassifier has a training score of 76.2% accuracy score on Const Fill\n",
            "Classifier: DecisionTreeClassifier has a test score of 76.7% accuracy score on Const Fill\n",
            "Classifier: GradientBoostingClassifier has a training score of 84.89999999999999% accuracy score on Const Fill\n",
            "Classifier: GradientBoostingClassifier has a test score of 80.0% accuracy score on Const Fill\n",
            "Classifier: RandomForestClassifier has a training score of 85.39999999999999% accuracy score on Mode Fill\n",
            "Classifier: RandomForestClassifier has a test score of 81.69999999999999% accuracy score on Mode Fill\n",
            "Classifier: DecisionTreeClassifier has a training score of 77.8% accuracy score on Mode Fill\n",
            "Classifier: DecisionTreeClassifier has a test score of 71.7% accuracy score on Mode Fill\n",
            "Classifier: GradientBoostingClassifier has a training score of 84.5% accuracy score on Mode Fill\n",
            "Classifier: GradientBoostingClassifier has a test score of 81.69999999999999% accuracy score on Mode Fill\n",
            "Classifier: RandomForestClassifier has a training score of 87.4% accuracy score on Const Fill Smote\n",
            "Classifier: RandomForestClassifier has a test score of 80.0% accuracy score on Const Fill Smote\n",
            "Classifier: DecisionTreeClassifier has a training score of 82.69999999999999% accuracy score on Const Fill Smote\n",
            "Classifier: DecisionTreeClassifier has a test score of 86.7% accuracy score on Const Fill Smote\n",
            "Classifier: GradientBoostingClassifier has a training score of 85.8% accuracy score on Const Fill Smote\n",
            "Classifier: GradientBoostingClassifier has a test score of 80.0% accuracy score on Const Fill Smote\n",
            "Classifier: RandomForestClassifier has a training score of 88.0% accuracy score on Mode Fill Smote\n",
            "Classifier: RandomForestClassifier has a test score of 83.3% accuracy score on Mode Fill Smote\n",
            "Classifier: DecisionTreeClassifier has a training score of 81.2% accuracy score on Mode Fill Smote\n",
            "Classifier: DecisionTreeClassifier has a test score of 83.3% accuracy score on Mode Fill Smote\n",
            "Classifier: GradientBoostingClassifier has a training score of 85.2% accuracy score on Mode Fill Smote\n",
            "Classifier: GradientBoostingClassifier has a test score of 81.69999999999999% accuracy score on Mode Fill Smote\n"
          ]
        }
      ]
    }
  ]
}